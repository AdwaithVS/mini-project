# -*- coding: utf-8 -*-
"""Phi4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-px5sA0UZsE9rfBp72_CUJhQZN0DSVbu

Installation
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth
# else:
#     # Do this only in Colab and Kaggle notebooks! Otherwise use pip install unsloth
#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton
#     !pip install --no-deps cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer
#     !pip install --no-deps unsloth

!pip install unsloth

"""Unsloth"""

from unsloth import FastLanguageModel  # FastVisionModel for LLMs
import torch
max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!
load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/Meta-Llama-3.1-8B-bnb-4bit",  # Llama-3.1 2x faster
    "unsloth/Mistral-Small-Instruct-2409",  # Mistral 22b 2x faster!
    "unsloth/Phi-4",  # Phi-4 2x faster!
    "unsloth/Phi-4-unsloth-bnb-4bit",  # Phi-4 Unsloth Dynamic 4-bit Quant
    "unsloth/gemma-2-9b-bnb-4bit",  # Gemma 2x faster!
    "unsloth/Qwen2.5-7B-Instruct-bnb-4bit"  # Qwen 2.5 2x faster!
    "unsloth/Llama-3.2-1B-bnb-4bit",  # NEW! Llama 3.2 models
    "unsloth/Llama-3.2-1B-Instruct-bnb-4bit",
    "unsloth/Llama-3.2-3B-bnb-4bit",
    "unsloth/Llama-3.2-3B-Instruct-bnb-4bit",
]  # More models at https://docs.unsloth.ai/get-started/all-our-models

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Phi-4",
    max_seq_length = max_seq_length,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

"""We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters."""

model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

import json

file_path = "/content/corrected_structured_mental_health_dataset.json"

# Load and print dataset structure
with open(file_path, "r", encoding="utf-8") as file:
    data = json.load(file)

print(json.dumps(data, indent=4))  # Pretty-print JSON structure

"""We now use the Phi-4 format for conversation style finetunes and loading dataset"""

from unsloth.chat_templates import get_chat_template
from datasets import load_dataset

# Load the Phi-4 chat template with correct mapping
tokenizer = get_chat_template(
    tokenizer,
    chat_template="phi-4",
    mapping={
        "role": "role",
        "content": "content",
        "user": "user",
        "assistant": "assistant"
    }
)

# Function to format the dataset using the chat template
def formatting_prompts_func(examples):
    convos = examples["conversations"]  # Ensure this key exists in your dataset
    texts = [
        tokenizer.apply_chat_template(
            convo, tokenize=False, add_generation_prompt=False
        )
        for convo in convos
    ]
    return {"text": texts}

# Load dataset using correct path
dataset = load_dataset("json", data_files="/content/corrected_structured_mental_health_dataset.json", split="train")

# Ensure dataset follows correct format
dataset = dataset.map(formatting_prompts_func, batched=True)

# Print a sample to verify
print(dataset[0])

!pip install datasets

"""Train the model"""

from trl import SFTTrainer
from transformers import TrainingArguments, DataCollatorForSeq2Seq
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),
    dataset_num_proc = 2,
    packing = False, # Can make training 5x faster for short sequences.
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        # num_train_epochs = 1, # Set this for 1 full training run.
        max_steps = 30,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none", # Use this for WandB etc
    ),
)

"""Mapping"""

from unsloth.chat_templates import standardize_sharegpt

dataset = standardize_sharegpt(dataset)
dataset = dataset.map(
    formatting_prompts_func,
    batched=True,
)

"""We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs."""

from unsloth.chat_templates import train_on_responses_only

trainer = train_on_responses_only(
    trainer,
    instruction_part="<|im_start|>user<|im_sep|>",
    response_part="<|im_start|>assistant<|im_sep|>",
)

"""We verify masking is actually done:"""

tokenizer.decode(trainer.train_dataset[5]["input_ids"])

space = tokenizer(" ", add_special_tokens = False).input_ids[0]
tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5]["labels"]])

# @title Show current memory stats
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")

trainer_stats = trainer.train()

# @title Show final memory and time stats
import torch

# Get the current GPU memory usage as the starting point
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)

# Get the maximum GPU memory available
max_memory = round(torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024, 3) # This line is added

# ... (Rest of the code remains the same) ...

used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_lora = round(used_memory - start_gpu_memory, 3)  # Now start_gpu_memory is defined
used_percentage = round(used_memory / max_memory * 100, 3) # Now max_memory is defined
lora_percentage = round(used_memory_for_lora / max_memory * 100, 3) # Now max_memory is defined
print(f"{trainer_stats.metrics['train_runtime']} seconds used for training.")
print(
    f"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training."
)
print(f"Peak reserved memory = {used_memory} GB.")
print(f"Peak reserved memory for training = {used_memory_for_lora} GB.")
print(f"Peak reserved memory % of max memory = {used_percentage} %.")
print(f"Peak reserved memory for training % of max memory = {lora_percentage} %.")

"""Inference"""

from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "phi-4",
)
FastLanguageModel.for_inference(model) # Enable native 2x faster inference

messages = [
    {"role": "user", "content": "Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,"},
]
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize = True,
    add_generation_prompt = True, # Must add for generation
    return_tensors = "pt",
).to("cuda")

outputs = model.generate(
    input_ids = inputs, max_new_tokens = 64, use_cache = True, temperature = 1.5, min_p = 0.1
)
tokenizer.batch_decode(outputs)

"""TextStreamer for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"""

FastLanguageModel.for_inference(model) # Enable native 2x faster inference

messages = [
    {"role": "user", "content": "Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,"},
]
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize = True,
    add_generation_prompt = True, # Must add for generation
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(
    input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,
    use_cache = True, temperature = 1.5, min_p = 0.1
)

"""<a name="Save"></a>
### Saving, loading finetuned models
"""

model.save_pretrained("lora_model")  # Local saving
tokenizer.save_pretrained("lora_model")
# model.push_to_hub("your_name/lora_model", token = "...") # Online saving
# tokenizer.push_to_hub("your_name/lora_model", token = "...") # Online saving

if False:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "lora_model", # YOUR MODEL YOU USED FOR TRAINING
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )
    FastLanguageModel.for_inference(model) # Enable native 2x faster inference

messages = [
    {"role": "user", "content": "Describe a tall tower in the capital of France."},
]
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize = True,
    add_generation_prompt = True, # Must add for generation
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(
    input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,
    use_cache = True, temperature = 1.5, min_p = 0.1
)

if False:
    # I highly do NOT suggest - use Unsloth if possible
    from peft import AutoPeftModelForCausalLM
    from transformers import AutoTokenizer

    model = AutoPeftModelForCausalLM.from_pretrained(
        "lora_model",  # YOUR MODEL YOU USED FOR TRAINING
        load_in_4bit=load_in_4bit,
    )
    tokenizer = AutoTokenizer.from_pretrained("lora_model")

from google.colab import userdata
# Merge to 16bit
if False: model.save_pretrained_merged("model", tokenizer, save_method = "merged_16bit",)
if False: model.push_to_hub_merged("hf/model", tokenizer, save_method = "merged_16bit", token = "")

# Merge to 4bit
if False: model.save_pretrained_merged("model", tokenizer, save_method = "merged_4bit",)
if False: model.push_to_hub_merged("hf/model", tokenizer, save_method = "merged_4bit", token = "")

# Just LoRA adapters
if False: model.save_pretrained_merged("model", tokenizer, save_method = "lora",)
if False: model.push_to_hub_merged("hf/model", tokenizer, save_method = "lora", token = "")

# Save to 8bit Q8_0
if False: model.save_pretrained_gguf("model", tokenizer,)
# Remember to go to https://huggingface.co/settings/tokens for a token!
# And change hf to your username!
if False: model.push_to_hub_gguf("hf/model", tokenizer, quantization_method = "q4_k_m", token = "")

# Save to 16bit GGUF
if False: model.save_pretrained_gguf("model", tokenizer, quantization_method = "f16")
if False: model.push_to_hub_gguf("hf/model", tokenizer, quantization_method = "f16", token = "")

# Save to q4_k_m GGUF
if False: model.save_pretrained_gguf("model", tokenizer, quantization_method = "q4_k_m")
if False: model.push_to_hub_gguf("hf/model", tokenizer, quantization_method = "q4_k_m", token = "")

# Save to multiple GGUF options - much faster if you want multiple!
if False:
    model.push_to_hub_gguf(
        "hf/model", # Change hf to your username!
        tokenizer,
        quantization_method = ["q4_k_m", "q8_0", "q5_k_m",],
        token = "", # Get a token at https://huggingface.co/settings/tokens
    )

"""Gradio 1"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install unsloth
# # Also get the latest nightly Unsloth!
# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git
# !pip install gradio

"""Gradio 2"""

import torch
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")  # Print the selected device

from unsloth import FastLanguageModel
import gradio as gr
from transformers import TextIteratorStreamer
from threading import Thread
import torch
import bitsandbytes as bnb

# Instead of loading the model again, reuse the existing model and tokenizer
# and enable inference mode.
# model, tokenizer = FastLanguageModel.from_pretrained(
#     model_name="/content/lora_model",
#     max_seq_length=2048,
#     dtype=None,
#     load_in_4bit=True,
#     device_map="auto", # Automatically distribute the model
#     offload_folder="offload", # Specify a folder for disk offloading
#     trust_remote_code=True,
# )

# Enable native 2x faster inference using the existing model
FastLanguageModel.for_inference(model)

!pip install --upgrade bitsandbytes

"""Gradio 3"""

from unsloth import FastLanguageModel
import gradio as gr
from transformers import TextIteratorStreamer, BitsAndBytesConfig  # ✅ Import correctly
from threading import Thread
import torch

# ✅ Detect if GPU is available
device = "cuda" if torch.cuda.is_available() else "cpu"

# ✅ Correct BitsAndBytes 4-bit Quantization Configuration
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,  # ✅ Use torch.float16 for compatibility
    bnb_4bit_use_double_quant=True,        # ✅ Enable double quantization for efficiency
    bnb_4bit_quant_type="nf4"              # ✅ Use Normal Float 4 (NF4)
)

# ✅ Load the fine-tuned model with BitsAndBytes
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="/content/lora_model",  # Change this to your actual fine-tuned model path
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True,
    device_map={"": 0} if torch.cuda.is_available() else {"": "cpu"},  # ✅ Manual offloading
    quantization_config=bnb_config  # ✅ Correct way to pass quantization config
)

# ✅ Enable inference mode
FastLanguageModel.for_inference(model)

# ✅ Phi-4 Prompt Format
PHI4_PROMPT = """<|im_start|>user<|im_sep|>{query}<|im_end|>
<|im_start|>assistant<|im_sep|>"""

def generate_response(query, max_new_tokens=512):
    """
    Generates a response using the fine-tuned model with the Phi-4 chat template.
    """
    prompt = PHI4_PROMPT.format(query=query)

    # Tokenize input
    inputs = tokenizer([prompt], return_tensors="pt").to(device)

    # Streaming response
    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    generation_kwargs = dict(
        inputs,
        streamer=streamer,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        top_p=0.95,
        top_k=50,
        temperature=0.7,
    )

    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    generated_text = ""
    for new_text in streamer:
        generated_text += new_text
    return generated_text.strip()

def chat(query, history):
    """
    Handles chatbot history and response generation.
    """
    history = history or []
    response = generate_response(query)
    history.append((query, response))
    return history, ""  # Return updated history & clear input box

# ✅ Build Gradio Chatbot UI
with gr.Blocks() as demo:
    gr.Markdown("<h1 style='text-align: center;'>🧠 Mental Health & Career Chatbot</h1>")

    chatbot = gr.Chatbot(height=500)
    msg = gr.Textbox(label="Enter your query", placeholder="Type your message here...")
    submit_button = gr.Button("Submit")
    clear_button = gr.Button("Clear Conversation")

    submit_button.click(chat, inputs=[msg, chatbot], outputs=[chatbot, msg])
    clear_button.click(lambda: ([], ""), outputs=[chatbot, msg], queue=False)

# ✅ Launch Gradio App
if __name__ == "__main__":
    demo.queue().launch(share=True, inbrowser=True)

"""Cache clear"""

import torch
torch.cuda.empty_cache()

!nvidia-smi

"""Crash"""

from unsloth import FastLanguageModel
import gradio as gr
from transformers import BitsAndBytesConfig
import torch
import gc
import os

# ✅ Step 1: Free GPU Memory Before Running
torch.cuda.empty_cache()
gc.collect()
os.system("fuser -k /dev/nvidia*")

# ✅ Step 2: Detect GPU Availability
device = "cuda" if torch.cuda.is_available() else "cpu"

# ✅ Step 3: Use Optimal Quantization for Tesla T4 (16GB VRAM)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,  # 🔹 Use bfloat16 for better T4 performance
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

# ✅ Step 4: Load Model with Offloading
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="/content/lora_model",
    max_seq_length=128,  # 🔹 Reduce sequence length to fit memory
    dtype=None,
    device_map="auto",  # 🔹 Automatically offloads layers to avoid memory issues
    quantization_config=bnb_config
)

# ✅ Step 5: Enable Inference Mode
FastLanguageModel.for_inference(model)

# ✅ Step 6: Optimize Model Execution (for less memory usage)
model = torch.compile(model)

# ✅ Step 7: Define Response Generation with Extra Memory Clearing
def generate_response(query, max_new_tokens=32):  # 🔹 Lower tokens to avoid memory crash
    """
    Generates a response using the fine-tuned model.
    """
    prompt = f"<|im_start|>user<|im_sep|>{query}<|im_end|>\n<|im_start|>assistant<|im_sep|>"

    inputs = tokenizer([prompt], return_tensors="pt").to(device)

    # ✅ Clear memory before generation
    torch.cuda.empty_cache()

    generation_kwargs = {
        "input_ids": inputs["input_ids"],
        "attention_mask": inputs["attention_mask"],
        "max_new_tokens": max_new_tokens,
        "do_sample": True,
        "top_p": 0.9,
        "top_k": 40,
        "temperature": 0.7,
    }

    output = model.generate(**generation_kwargs)

    # ✅ Clear memory after generation
    torch.cuda.empty_cache()

    return tokenizer.decode(output[0], skip_special_tokens=True).strip()

# ✅ Step 8: Chat Function
def chat(query, history):
    history = history or []
    response = generate_response(query)
    history.append((query, response))
    return history, ""

# ✅ Step 9: Build Gradio Chatbot UI
with gr.Blocks() as demo:
    gr.Markdown("<h1 style='text-align: center;'>🧠 Mental Health & Career Chatbot</h1>")

    chatbot = gr.Chatbot(height=500)
    msg = gr.Textbox(label="Enter your query", placeholder="Type your message here...")
    submit_button = gr.Button("Submit")
    clear_button = gr.Button("Clear Conversation")

    submit_button.click(chat, inputs=[msg, chatbot], outputs=[chatbot, msg])
    clear_button.click(lambda: ([], ""), outputs=[chatbot, msg], queue=False)

# ✅ Step 10: Launch Gradio App
if __name__ == "__main__":
    demo.queue().launch(share=True, inbrowser=True)